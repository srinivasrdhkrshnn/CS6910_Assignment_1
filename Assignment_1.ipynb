{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7cff98faec14450dabd714823d4aeeca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3f33a1091a0d4a29927274011789b4bd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2597ae7ff0df4021a7b84ee4fd7545a4",
              "IPY_MODEL_9fb4b710580040449f5de8b8a43ecf2b"
            ]
          }
        },
        "3f33a1091a0d4a29927274011789b4bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2597ae7ff0df4021a7b84ee4fd7545a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_ace4e1fe861048f597908cf9a667ec7d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f84a0f491480419c98242b6ba1afde38"
          }
        },
        "9fb4b710580040449f5de8b8a43ecf2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8a5e44906a504e1eb4fbccb5dae2f6ba",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_948da8ab43ec4c05a7f7d92e80c61c72"
          }
        },
        "ace4e1fe861048f597908cf9a667ec7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f84a0f491480419c98242b6ba1afde38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8a5e44906a504e1eb4fbccb5dae2f6ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "948da8ab43ec4c05a7f7d92e80c61c72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srinivasrdhkrshnn/CS6910_Assignment_1/blob/main/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRXXPzPKDf4V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fd9a9a8-8d89-4977-f80f-43a55699e973"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/ae/79374d2b875e638090600eaa2a423479865b7590c53fb78e8ccf6a64acb1/wandb-0.10.22-py2.py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 41.9MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 32.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.0.0)\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.2MB/s \n",
            "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=361928b83f1daeaf5d74e22e1d9b3f012c790f2458225b04d96277f417f08c5c\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=d36329ecffc8211ffafb2aa4c653718fa8d8dcf20d340a1837f6582cfc5cb4ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: subprocess32, configparser, pathtools, sentry-sdk, docker-pycreds, smmap, gitdb, GitPython, shortuuid, wandb\n",
            "Successfully installed GitPython-3.1.14 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.5 pathtools-0.1.2 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-3.0.5 subprocess32-3.5.4 wandb-0.10.22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KptVZx1fm3E1"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from keras.datasets import fashion_mnist\r\n",
        "from keras.datasets import mnist\r\n",
        "import wandb\r\n",
        "\r\n",
        "# load dataset\r\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\r\n",
        "class_type = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'] \r\n",
        "\r\n",
        "proj_name='CS6910_ass1'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "JUPA7aDORAu_",
        "outputId": "6f9d029f-d89a-49c3-b372-84c3d058f1da"
      },
      "source": [
        "\r\n",
        "print(\"Sample Images for each Class :\")\r\n",
        "class_list=list()\r\n",
        "wandb.init(project=proj_name)\r\n",
        "for i in range(10):\r\n",
        "  plt.subplot(2,5,i+1)\r\n",
        "  for j in range(len(y_train)):\r\n",
        "    if y_train[j] == i :\r\n",
        "        wandb.log({\"img\": [wandb.Image(x_train[j], caption=class_type[y_train[j]])]})\r\n",
        "        class_list.append(class_type[y_train[j]])\r\n",
        "        break      "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample Images for each Class :\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrohithd\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.22<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">feasible-lion-146</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/rohithd/CS6910_ass1\" target=\"_blank\">https://wandb.ai/rohithd/CS6910_ass1</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/rohithd/CS6910_ass1/runs/6fxqs7dm\" target=\"_blank\">https://wandb.ai/rohithd/CS6910_ass1/runs/6fxqs7dm</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210315_101852-6fxqs7dm</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZ0U8UVcl9ZW"
      },
      "source": [
        "def activate(x,activation):                                                           # Hidden layer activation function                                  \n",
        "\n",
        "  if activation == \"sigmoid\":\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "  elif activation == \"tanh\": \n",
        "    return np.tanh(x) \n",
        "\n",
        "  elif activation == \"relu\": \n",
        "    return x * (x > 0) \n",
        "\n",
        "def softmax(x):                                                                       # Output activation function\n",
        "    return np.exp(x) / np.sum(np.exp(x)) \n",
        "\n",
        "def feed_forward(x,parameters,sizes,activation):                                # feed-forward data through the network to estimate output\n",
        "  \n",
        "  H = {}\n",
        "  A={}\n",
        "  H[0] = x\n",
        " \n",
        "  for i in range(1,len(sizes)-1):\n",
        "    W = parameters[\"W\"+str(i)]\n",
        "    b = parameters[\"b\"+str(i)]\n",
        "    A[i] = np.dot(W,H[i-1])+b\n",
        "    H[i] = activate(A[i],activation)\n",
        "    \n",
        "  W = parameters[\"W\"+str(len(sizes)-1)]\n",
        "  b = parameters[\"b\"+str(len(sizes)-1)]\n",
        "  A[len(sizes)-1] = np.dot(W,H[len(sizes)-2])+b\n",
        "  #print(\"A:\",A[2],\"H:\",H[3],len(sizes))\n",
        "\n",
        "  y_hat = softmax(A[len(sizes)-1])\n",
        "  #print(y_hat)\n",
        "  \n",
        "  return y_hat,A,H\n",
        "\n",
        "def loss_compute(y,y_hat,parameters,loss_type):                                               # function to compute the loss/error (both squared error and cross entropy)\n",
        "\n",
        "  if (loss_type == \"squared_error\"):\n",
        "    error = np.sum((y-y_hat)**2)/(2*len(y))\n",
        "  elif (loss_type == \"cross_entropy\") :\n",
        "    error = -1*np.sum(np.multiply(y,np.log(y_hat)))/len(y)\n",
        "\t\t\n",
        "  reg_error = 0.0                                                                        # account for regularization to avoid overfit of data - L2 norm regularization\n",
        "  for i in range(1,len(sizes)) :\n",
        "    reg_error = reg_error + (reg/2)*(np.sum(np.square(parameters[\"W\"+str(i)]))) \n",
        "  error = error + reg_error\n",
        "\n",
        "  return error\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe9Q5RfeoV34"
      },
      "source": [
        "def derivative(x,activation):                                                            # function to compute the derivative of hidden layer activation fn\n",
        "\n",
        "  if activation == \"sigmoid\":\n",
        "    return activate(x,\"sigmoid\")*(1-activate(x,\"sigmoid\"))\n",
        "  elif activation == \"tanh\": \n",
        "    return 1. - x * x \n",
        "  elif activation == \"relu\": \n",
        "    return 1. * (x > 0)\n",
        "\n",
        "def grad_init(sizes):\n",
        "  \n",
        "  grads={}\n",
        "  layers=len(sizes)\n",
        "  for i in range(1,layers):\n",
        "    grads[\"dW\" + str(i)] = np.zeros((sizes[i], sizes[i-1]))\n",
        "    grads[\"db\" + str(i)] = np.zeros((sizes[i],1))\n",
        "\n",
        "  return grads  \n",
        "\n",
        "\n",
        "def back_prop(X,Y,Y_hat,prev_grad,A,H,parameters,sizes,loss_type,activation,reg) :               # back-propogation rule to compute the gradients of activation, pre-activation and parameters\n",
        "  \n",
        "  new_grad = {}\n",
        "  grads = {}\n",
        "  # grads = {\"dH0\":np.zeros((input_size,1)),\"dA0\":np.zeros((input_size,1))}\n",
        "  for i in range(1,len(sizes)):\n",
        "    grads[\"dW\" + str(i)] = np.zeros((sizes[i], sizes[i-1]))\n",
        "    grads[\"db\" + str(i)] = np.zeros((sizes[i],1))\n",
        "    grads[\"dA\" + str(i)] = np.zeros((sizes[i],1))\n",
        "    grads[\"dH\" + str(i)] = np.zeros((sizes[i],1))\n",
        "\n",
        "  if loss_type == \"squared_error\":\n",
        "    grads[\"dH\"+str(len(sizes)-1)] = (Y_hat-Y) \n",
        "    grads[\"dA\"+str(len(sizes)-1)] = (Y_hat - Y)*Y_hat - Y_hat*(np.dot(np.transpose((Y_hat-Y)), Y_hat))\n",
        "\n",
        "  elif loss_type==\"cross_entropy\" :\n",
        "    grads[\"dH\"+str(len(sizes)-1)] = -(Y/Y_hat) \n",
        "    grads[\"dA\"+str(len(sizes)-1)] = -(Y-Y_hat)\n",
        "\n",
        "  for i in range(len(sizes)-1, 0, -1):\n",
        "    grads[\"dW\" + str(i)] = np.dot(grads[\"dA\" + str(i)], np.transpose(H[i-1]))\n",
        "    grads[\"db\" + str(i)] = grads[\"dA\" + str(i)] \n",
        "    if i>1 :\n",
        "      grads[\"dH\" + str(i-1)] = np.dot(np.transpose(parameters[\"W\" + str(i)]), grads[\"dA\" + str(i)])\n",
        "      grads[\"dA\" + str(i-1)] = np.multiply((grads[\"dH\" + str(i-1)]),derivative(A[i-1],activation))\n",
        "    \n",
        "  for i in range(1,len(sizes)):\n",
        "    new_grad[\"dW\" + str(i)] = grads[\"dW\" + str(i)] + prev_grad[\"dW\" + str(i)]\n",
        "    new_grad[\"db\" + str(i)] = grads[\"db\" + str(i)] + prev_grad[\"db\" + str(i)]\n",
        "    \n",
        "  return new_grad  "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OplHJZ91N5C0"
      },
      "source": [
        "# Initializations\r\n",
        "\r\n",
        "# Data\r\n",
        "X_train = np.array(x_train.reshape(x_train.shape[0], 784,1))         # reshape 2-D data to 1-D\r\n",
        "X_test = np.array(x_test.reshape(x_test.shape[0], 784,1))            # reshape 2-D data to 1-D\r\n",
        "\r\n",
        "def normalize_data(x):                                               # normalize input data\r\n",
        "  x_norm = x.astype('float32')\r\n",
        "  x_norm = x_norm / 255.0  \r\n",
        "  return x_norm \r\n",
        "\r\n",
        "X_train = normalize_data(X_train)\r\n",
        "X_val = X_train[-6000:]                                             # validation set input\r\n",
        "X_train = X_train[0:54000]                                          # training set input\r\n",
        "X_test = normalize_data(X_test)                                     # test set input\r\n",
        "\r\n",
        "\r\n",
        "Y_train = np.zeros([len(y_train),10,1])\r\n",
        "Y_test = np.zeros([len(y_test),10,1])\r\n",
        "\r\n",
        "for i in range(len(y_train)):                                        # convert y from just a class number to an indicator vector (10x1)\r\n",
        "  y = np.zeros([10, 1])\r\n",
        "  y[y_train[i]] = 1.0\r\n",
        "  Y_train[i] = y\r\n",
        "\r\n",
        "Y_val = Y_train[-6000:]                                              # validation set output\r\n",
        "Y_train = Y_train[0:54000]                                           # training set output\r\n",
        "\r\n",
        "for i in range(len(y_test)):                                         # convert y from just a class number to an indicator vector (10x1)\r\n",
        "  y = np.zeros([10, 1])\r\n",
        "  y[y_test[i]] = 1.0\r\n",
        "  Y_test[i] = y                                                      # test set output"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYuLb65aZAVr"
      },
      "source": [
        "def network_init(sizes,w_init):                            # function to initialize weights and biases\r\n",
        "  parameters = {}\r\n",
        "  for i in range(1, len(sizes)):\r\n",
        "    if w_init == \"xavier\" :\r\n",
        "      parameters[\"W\" + str(i)] = np.random.randn(sizes[i], sizes[i-1])*np.sqrt(2./(sizes[i] + sizes[i-1]))\r\n",
        "      parameters[\"b\" + str(i)] = np.zeros((sizes[i],1))\r\n",
        "    elif w_init == \"random\" :\r\n",
        "      parameters[\"W\" + str(i)] = 0.01*np.random.randn(sizes[i], sizes[i-1])\r\n",
        "      parameters[\"b\" + str(i)] = 0.01*np.random.randn(sizes[i],1)\r\n",
        "\r\n",
        "  return parameters  \r\n",
        "\r\n",
        "def update_init(sizes) :                                  # function to initialize update dictionary that changes the weights and biases\r\n",
        "  update = {}\r\n",
        "  for i in range(1,len(sizes)):\r\n",
        "   update[\"W\"+str(i)] = np.zeros((sizes[i],sizes[i-1]))\r\n",
        "   update[\"b\"+str(i)] = np.zeros((sizes[i],1))\r\n",
        "\r\n",
        "  return update"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJTGRW9sb2C7"
      },
      "source": [
        "def loss(y,y_hat,loss_type):                                # function to compute the loss/error (both squared error and cross entropy)\r\n",
        "  l = 0\r\n",
        "  if (loss_type == \"squared_error\"):\r\n",
        "    l = np.sum((y-y_hat)**2)/(2*len(y))\r\n",
        "  elif (loss_type == \"cross_entropy\") :\r\n",
        "    l = -1*np.sum(np.multiply(y,np.log(y_hat)))/len(y)\r\n",
        "  return l \r\n",
        "\r\n",
        "def calcAccLoss(parameters,xArr,yArr,sizes,loss_type,activation):          #function to calculate accuracy and total loss of a model\r\n",
        "  acc=0.0\r\n",
        "  lossVal=0.0\r\n",
        "  for x,y in zip(xArr,yArr):\r\n",
        "    y_hat= feed_forward(x,parameters,sizes,activation)[0]\r\n",
        "    if y_hat.argmax()==y.argmax():\r\n",
        "      acc+=1\r\n",
        "    lossVal+=loss(y,y_hat,loss_type)\r\n",
        "  acc=acc/len(xArr)\r\n",
        "  return (acc,lossVal)\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "getumZEKyShg"
      },
      "source": [
        "#momentum Gradient descent\n",
        "def momentum_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=False) :\n",
        "\n",
        "   steps=0                                                                                      #used to count number of updates to parameters\n",
        "   parameters = network_init(sizes,w_init)\n",
        "   update = update_init(sizes)\n",
        "   gamma = 0.9\n",
        "   for n in range(n_epoch):\n",
        "\n",
        "     for j in range(0, X_train.shape[0], minibatch_size):                                       #minibatch division \n",
        "       X_mini = X_train[j:j + minibatch_size]\n",
        "       Y_mini = Y_train[j:j + minibatch_size]\n",
        "       grads = grad_init(sizes)\n",
        "\n",
        "       for x,y in zip(X_mini,Y_mini):\n",
        "         y_hat,A,H = feed_forward(x,parameters,sizes,activation)                                 \n",
        "         grads = back_prop(x,y,y_hat,grads,A,H,parameters,sizes,loss_type,activation,reg)\n",
        "\n",
        "       for i in range(1,len(sizes)) :                                                           #updating the parameters\n",
        "         update[\"W\"+str(i)] = gamma*update[\"W\"+str(i)] + lr*grads[\"dW\"+str(i)]                  \n",
        "         update[\"b\"+str(i)] = gamma*update[\"b\"+str(i)] + lr*grads[\"db\"+str(i)]\n",
        "         parameters[\"W\"+str(i)] = (1-lr*reg)*parameters[\"W\"+str(i)] - update[\"W\"+str(i)]\n",
        "         parameters[\"b\"+str(i)] = (1-lr*reg)*parameters[\"b\"+str(i)] - update[\"b\"+str(i)]\n",
        "       \n",
        "       steps=steps+1\n",
        "       if steps==10000:                                                                         #log every 10000 updates\n",
        "        acc,lossTot=calcAccLoss(parameters,X_train,Y_train,sizes,loss_type,activation)          #calculating accuracy and loss\n",
        "        accVal,lossTotVal=calcAccLoss(parameters,X_val,Y_val,sizes,loss_type,activation)\n",
        "\n",
        "        wandb.log({\"Accuracy\":acc,\"Loss\":lossTot,\"Accuracy_val\":accVal,\"Loss_val\":lossTotVal,\"Epoch\":n,\"n_datatrain\":j + minibatch_size+n*54000}) #logging\n",
        "        steps=0\n",
        "  \n",
        "   return parameters        \n",
        "\n",
        "#Nesterov acceleracted GD\n",
        "def nesterov_accelerated_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=False) :\n",
        "  steps=0\n",
        "  parameters = network_init(sizes,w_init)\n",
        "  update = update_init(sizes)\n",
        "  gamma = 0.9\n",
        "  for n in range(n_epoch):\n",
        "\n",
        "    for j in range(0, X_train.shape[0], minibatch_size):\n",
        "      X_mini = X_train[j:j + minibatch_size]\n",
        "      Y_mini = Y_train[j:j + minibatch_size]\n",
        "      \n",
        "\n",
        "      grads = grad_init(sizes)\n",
        "      for i in range(1,len(sizes)):                                                       #perform update before back propogation\n",
        "        update[\"W\"+str(i)] = gamma*update[\"W\"+str(i)]\n",
        "        update[\"b\"+str(i)] = gamma*update[\"b\"+str(i)]\n",
        "        parameters[\"W\"+str(i)] = (1-lr*reg)*parameters[\"W\"+str(i)] - update[\"W\"+str(i)]\n",
        "        parameters[\"b\"+str(i)] = (1-lr*reg)*parameters[\"b\"+str(i)] - update[\"b\"+str(i)]        \n",
        "\n",
        "      for x,y in zip(X_mini,Y_mini):\n",
        "        y_hat,A,H = feed_forward(x,parameters,sizes,activation)\n",
        "        grads = back_prop(x,y,y_hat,grads,A,H,parameters,sizes,loss_type,activation,reg)\n",
        "        \n",
        "      for k in range(1,len(sizes)) :\n",
        "        update[\"W\"+str(k)] = gamma*update[\"W\"+str(k)] + lr*grads[\"dW\"+str(k)]\n",
        "        update[\"b\"+str(k)] = gamma*update[\"b\"+str(k)] + lr*grads[\"db\"+str(k)]\n",
        "        parameters[\"W\"+str(k)] = (1-lr*reg)*parameters[\"W\"+str(k)] - update[\"W\"+str(k)]\n",
        "        parameters[\"b\"+str(k)] = (1-lr*reg)*parameters[\"b\"+str(k)] - update[\"b\"+str(k)]\n",
        "      steps=steps+1\n",
        "      if steps==10000:\n",
        "        if log:\n",
        "          acc,lossTot=calcAccLoss(parameters,X_train,Y_train,sizes,loss_type,activation)\n",
        "          accVal,lossTotVal=calcAccLoss(parameters,X_val,Y_val,sizes,loss_type,activation)\n",
        "\n",
        "          wandb.log({\"Accuracy\":acc,\"Loss\":lossTot,\"Accuracy_val\":accVal,\"Loss_val\":lossTotVal,\"Epoch\":n,\"n_datatrain\":j + minibatch_size+n*54000})\n",
        "        steps=0   \n",
        "    \n",
        "  return parameters\n",
        "\n",
        "#stochastic GD\n",
        "def stochastic_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=False) :\n",
        "\n",
        "  steps=0\n",
        "  parameters = network_init(sizes,w_init)\n",
        "  update = update_init(sizes)\n",
        "  gamma = 0.9\n",
        "  for n in range(n_epoch):\n",
        "\n",
        "    for j in range(0, X_train.shape[0], minibatch_size):                                        #minibatch division\n",
        "      X_mini = X_train[j:j + minibatch_size]\n",
        "      Y_mini = Y_train[j:j + minibatch_size]\n",
        "      \n",
        "      grads = grad_init(sizes)\n",
        "      for x,y in zip(X_mini,Y_mini):\n",
        "        y_hat,A,H = feed_forward(x,parameters,sizes,activation)\n",
        "        grads = back_prop(x,y,y_hat,grads,A,H,parameters,sizes,loss_type,activation,reg)\n",
        "        \n",
        "      for i in range(1,len(sizes)-1) :                                                          #updating the parameters\n",
        "        parameters[\"W\"+str(i)] = (1-lr*reg)*parameters[\"W\"+str(i)] - lr*grads[\"dW\"+str(i)]\n",
        "        parameters[\"b\"+str(i)] = (1-lr*reg)*parameters[\"b\"+str(i)] - lr*grads[\"db\"+str(i)]\n",
        "      steps=steps+1\n",
        "      if steps==10000:\n",
        "        if log:\n",
        "          acc,lossTot=calcAccLoss(parameters,X_train,Y_train,sizes,loss_type,activation)\n",
        "          accVal,lossTotVal=calcAccLoss(parameters,X_val,Y_val,sizes,loss_type,activation)\n",
        "\n",
        "          wandb.log({\"Accuracy\":acc,\"Loss\":lossTot,\"Accuracy_val\":accVal,\"Loss_val\":lossTotVal,\"Epoch\":n,\"n_datatrain\":j + minibatch_size+n*54000})\n",
        "        steps=0   \n",
        "\n",
        "  return parameters        \n",
        "\n",
        "#rmsprop GD\n",
        "def rmsprop_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=False) :\n",
        "  \n",
        "  steps=0\n",
        "  parameters = network_init(sizes,w_init)\n",
        "  update = update_init(sizes)\n",
        "  v = update_init(sizes)\n",
        "\n",
        "  betal = 0.99 #check this\n",
        "  eps = 1e-8\n",
        "\n",
        "  for n in range(n_epoch):\n",
        "\n",
        "    for j in range(0, X_train.shape[0], minibatch_size):\n",
        "      X_mini = X_train[j:j + minibatch_size]\n",
        "      Y_mini = Y_train[j:j + minibatch_size]\n",
        "      \n",
        "      grads = grad_init(sizes)\n",
        "      for x,y in zip(X_mini,Y_mini):\n",
        "        y_hat,A,H = feed_forward(x,parameters,sizes,activation)\n",
        "        grads = back_prop(x,y,y_hat,grads,A,H,parameters,sizes,loss_type,activation,reg)\n",
        "\n",
        "      for i in range(1,len(sizes)-1) :                                                                   #updating the parameters\n",
        "        v[\"W\"+str(i)] = betal*v[\"W\"+str(i)] + (1-betal)*grads[\"dW\"+str(i)]**2                            #v_w update  \n",
        "        v[\"b\"+str(i)] = betal*v[\"b\"+str(i)] + (1-betal)*grads[\"db\"+str(i)]**2                            #v_b update\n",
        "\n",
        "        update[\"W\"+str(i)]=lr*np.multiply(np.reciprocal(np.sqrt(v[\"W\"+str(i)]+eps)),grads[\"dW\"+str(i)])                 \n",
        "        update[\"b\"+str(i)]=lr*np.multiply(np.reciprocal(np.sqrt(v[\"b\"+str(i)]+eps)),grads[\"db\"+str(i)])\n",
        "        \n",
        "        parameters[\"W\"+str(i)] = (1-lr*reg)*parameters[\"W\"+str(i)] - update[\"W\"+str(i)]\n",
        "        parameters[\"b\"+str(i)] = (1-lr*reg)*parameters[\"b\"+str(i)] - update[\"b\"+str(i)]\n",
        "      steps=steps+1\n",
        "      if steps==10000:\n",
        "        if log:\n",
        "          acc,lossTot=calcAccLoss(parameters,X_train,Y_train,sizes,loss_type,activation)\n",
        "          accVal,lossTotVal=calcAccLoss(parameters,X_val,Y_val,sizes,loss_type,activation)\n",
        "\n",
        "          wandb.log({\"Accuracy\":acc,\"Loss\":lossTot,\"Accuracy_val\":accVal,\"Loss_val\":lossTotVal,\"Epoch\":n,\"n_datatrain\":j + minibatch_size+n*54000})\n",
        "        steps=0   \n",
        "\n",
        "  return parameters          \n",
        "\n",
        "#Adam GD\n",
        "def adam_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=False) :\n",
        "  steps=0\n",
        "  parameters = network_init(sizes,w_init)\n",
        "  update = update_init(sizes)\n",
        "  m = update_init(sizes)\n",
        "  v = update_init(sizes)\n",
        "\n",
        "  beta1 = 0.9\n",
        "  beta2 = 0.999\n",
        "  eps = 1e-8\n",
        "  for n in range(n_epoch):\n",
        "\n",
        "    for j in range(0, X_train.shape[0], minibatch_size):\n",
        "      X_mini = X_train[j:j + minibatch_size]\n",
        "      Y_mini = Y_train[j:j + minibatch_size]\n",
        "      \n",
        "      grads = grad_init(sizes)                                          \n",
        "      for x,y in zip(X_mini,Y_mini):\n",
        "        y_hat,A,H = feed_forward(x,parameters,sizes,activation)\n",
        "        grads = back_prop(x,y,y_hat,grads,A,H,parameters,sizes,loss_type,activation,reg)\n",
        "        \n",
        "      for i in range(1,len(sizes)-1) :                                                    #updating the parameters\n",
        "        m[\"W\"+str(i)] = beta1*m[\"W\"+str(i)] + (1-beta1)*grads[\"dW\"+str(i)]                #m_w update\n",
        "        m[\"b\"+str(i)] = beta1*m[\"b\"+str(i)] + (1-beta1)*grads[\"db\"+str(i)]                #m_b update\n",
        "\n",
        "        v[\"W\"+str(i)] = beta2*v[\"W\"+str(i)] + (1-beta2)*grads[\"dW\"+str(i)]**2             #v_w update    \n",
        "        v[\"b\"+str(i)] = beta2*v[\"b\"+str(i)] + (1-beta2)*grads[\"db\"+str(i)]**2             #v_b update\n",
        "\n",
        "        #cumulative average\n",
        "        m_w_hat = m[\"W\"+str(i)]/(1-np.power(beta1,n+1))                                   \n",
        "        m_b_hat = m[\"b\"+str(i)]/(1-np.power(beta1,n+1))\n",
        "        v_w_hat = v[\"W\"+str(i)]/(1-np.power(beta2,n+1))\n",
        "        v_b_hat = v[\"b\"+str(i)]/(1-np.power(beta2,n+1))\n",
        "\n",
        "\n",
        "        update[\"W\"+str(i)]=lr*np.multiply(np.reciprocal(np.sqrt(v_w_hat+eps)),m_w_hat)\n",
        "        update[\"b\"+str(i)]=lr*np.multiply(np.reciprocal(np.sqrt(v_b_hat+eps)),m_b_hat)\n",
        "\n",
        "        parameters[\"W\"+str(i)] = (1-lr*reg)*parameters[\"W\"+str(i)] - update[\"W\"+str(i)]\n",
        "        parameters[\"b\"+str(i)] = (1-lr*reg)*parameters[\"b\"+str(i)] - update[\"b\"+str(i)]\n",
        "      \n",
        "    \n",
        "      steps=steps+1\n",
        "      if steps==10000:\n",
        "        if log:\n",
        "          acc,lossTot=calcAccLoss(parameters,X_train,Y_train,sizes,loss_type,activation)\n",
        "          accVal,lossTotVal=calcAccLoss(parameters,X_val,Y_val,sizes,loss_type,activation)\n",
        "\n",
        "          wandb.log({\"Accuracy\":acc,\"Loss\":lossTot,\"Accuracy_val\":accVal,\"Loss_val\":lossTotVal,\"Epoch\":n,\"n_datatrain\":j + minibatch_size+n*54000})\n",
        "        steps=0     \n",
        "\n",
        "  return parameters   \n",
        "\n",
        "\n",
        "#Nadam GD\n",
        "def nadam_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=False) :\n",
        "  steps=0\n",
        "  parameters = network_init(sizes,w_init)\n",
        "  update = update_init(sizes)\n",
        "  m = update_init(sizes)\n",
        "  v = update_init(sizes)\n",
        "\n",
        "  beta1 = 0.9\n",
        "  beta2 = 0.999\n",
        "  eps = 1e-8\n",
        "\n",
        "  for n in range(n_epoch):\n",
        "\n",
        "    for j in range(0, X_train.shape[0], minibatch_size):\n",
        "      X_mini = X_train[j:j + minibatch_size]\n",
        "      Y_mini = Y_train[j:j + minibatch_size]\n",
        "      grads = grad_init(sizes)\n",
        "\n",
        "      for x,y in zip(X_mini,Y_mini):\n",
        "        y_hat,A,H = feed_forward(x,parameters,sizes,activation)\n",
        "        grads = back_prop(x,y,y_hat,grads,A,H,parameters,sizes,loss_type,activation,reg)\n",
        "\n",
        "      for i in range(1,len(sizes)-1) :                                                    #updating the parameters\n",
        "        m[\"W\"+str(i)] = beta1*m[\"W\"+str(i)] + (1-beta1)*grads[\"dW\"+str(i)]                #m_w update\n",
        "        m[\"b\"+str(i)] = beta1*m[\"b\"+str(i)] + (1-beta1)*grads[\"db\"+str(i)]                #m_b update\n",
        "\n",
        "        v[\"W\"+str(i)] = beta2*v[\"W\"+str(i)] + (1-beta2)*grads[\"dW\"+str(i)]**2             #v_w update    \n",
        "        v[\"b\"+str(i)] = beta2*v[\"b\"+str(i)] + (1-beta2)*grads[\"db\"+str(i)]**2             #v_b update\n",
        "\n",
        "        #cumulative average\n",
        "        m_w_hat = m[\"W\"+str(i)]/(1-np.power(beta1,n+1))                                   \n",
        "        m_b_hat = m[\"b\"+str(i)]/(1-np.power(beta1,n+1))\n",
        "        v_w_hat = v[\"W\"+str(i)]/(1-np.power(beta2,n+1))\n",
        "        v_b_hat = v[\"b\"+str(i)]/(1-np.power(beta2,n+1))\n",
        "\n",
        "\n",
        "        update[\"W\"+str(i)]=lr*np.multiply(np.reciprocal(np.sqrt(v_w_hat+eps)),(beta1*m_w_hat+(1-beta1)*grads[\"dW\"+str(i)]))*(1/(1-np.power(beta1,n+1)))\n",
        "        update[\"b\"+str(i)]=lr*np.multiply(np.reciprocal(np.sqrt(v_b_hat+eps)),(beta1*m_b_hat+(1-beta1)*grads[\"db\"+str(i)]))*(1/(1-np.power(beta1,n+1)))\n",
        "\n",
        "        parameters[\"W\"+str(i)] = (1-lr*reg)*parameters[\"W\"+str(i)] - update[\"W\"+str(i)]\n",
        "        parameters[\"b\"+str(i)] = (1-lr*reg)*parameters[\"b\"+str(i)] - update[\"b\"+str(i)]\n",
        "      steps=steps+1\n",
        "      if steps==10000:\n",
        "        if log:\n",
        "          acc,lossTot=calcAccLoss(parameters,X_train,Y_train,sizes,loss_type,activation)\n",
        "          accVal,lossTotVal=calcAccLoss(parameters,X_val,Y_val,sizes,loss_type,activation)\n",
        "\n",
        "          wandb.log({\"Accuracy\":acc,\"Loss\":lossTot,\"Accuracy_val\":accVal,\"Loss_val\":lossTotVal,\"Epoch\":n,\"n_datatrain\":j + minibatch_size+n*54000})\n",
        "        steps=0   \n",
        "\n",
        "  return parameters   "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SvODi-MsNvw"
      },
      "source": [
        "#function to select optimizer\r\n",
        "def do_GD(X_train,Y_train,optimizer,activation,hl_size,input_size,output_size,n_epoch,lr,reg,w_init,loss_type,minibatch_size=1,logging=False):\r\n",
        "  sizes = hl_size.copy() \r\n",
        "  sizes.insert(0,input_size)\r\n",
        "  sizes.append(output_size)\r\n",
        "\r\n",
        "  if optimizer==\"sgd\":\r\n",
        "    return(stochastic_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=logging))\r\n",
        "  elif optimizer==\"momentum\":\r\n",
        "    return(momentum_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=logging))\r\n",
        "  elif optimizer==\"nesterov\":\r\n",
        "    return(nesterov_accelerated_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=logging))\r\n",
        "  elif optimizer==\"rmsprop\":\r\n",
        "    return(rmsprop_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=logging))\r\n",
        "  elif optimizer==\"adam\":\r\n",
        "    return(adam_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=logging))\r\n",
        "  elif optimizer==\"nadam\":\r\n",
        "    return(nadam_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=logging))\r\n",
        "  "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOn63t--FAVJ"
      },
      "source": [
        "#training function to sweep with wandb\r\n",
        "def train():\r\n",
        "\r\n",
        "  hyperparameter_defaults=dict(\r\n",
        "      input_size = 784,                                       \r\n",
        "      output_size = 10,                                                    \r\n",
        "      n_epoch = 5,                                            \r\n",
        "      n_hiddenlayer = 3,                               \r\n",
        "      hl= [64,64,64],\r\n",
        "      reg = 0.0005,      \r\n",
        "      lr = 1e-3,                                              \r\n",
        "      optimizer = \"momentum\",                      \r\n",
        "      batch_size = 64,       \r\n",
        "      initialization = \"xavier\",      \r\n",
        "      loss_type = \"cross_entropy\" \r\n",
        "      \r\n",
        "  )\r\n",
        "\r\n",
        "  wandb.init(config=hyperparameter_defaults)\r\n",
        "\r\n",
        "  config=wandb.config\r\n",
        "  output_size=10\r\n",
        "  input_size = 784                                      \r\n",
        "  config.hl=[config.hl_size for i in range(config.n_hiddenlayer)]   #hidden layer sizes array creation\r\n",
        "  parameters=do_GD(X_train,Y_train,config.optimizer,config.activation,config.hl,config.input_size,config.output_size,config.n_epoch,config.lr,config.reg,config.initialization,config.loss_type,config.batch_size,logging=True)\r\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88hU77dlo4-O"
      },
      "source": [
        "#sweep dictionary\r\n",
        "sweep_config={\r\n",
        "    'method':'bayes',\r\n",
        "    'metric':{\r\n",
        "        'name':'accuracy',\r\n",
        "        'goal':'maximize'},\r\n",
        "\r\n",
        "}\r\n",
        "\r\n",
        "parameters_dict={\r\n",
        "    'optimizer':{\r\n",
        "        'values':['nadam','sgd', 'momentum', 'nesterov', 'rmsprop', 'adam']\r\n",
        "    },\r\n",
        "    'lr':{\r\n",
        "        'values':[1e-3,1e-5]\r\n",
        "    },\r\n",
        "    'reg':{\r\n",
        "        'values':[5e-4,0,5e-1]\r\n",
        "    },\r\n",
        "    'n_hiddenlayer':{\r\n",
        "        'values':[3,4,5]\r\n",
        "    },\r\n",
        "    'hl_size':{\r\n",
        "      'values':[128,32,64]  \r\n",
        "    },\r\n",
        "    'batch_size':{\r\n",
        "        'values':[64,32,128]\r\n",
        "    },\r\n",
        "    'loss_type':{\r\n",
        "        'values':['cross_entropy','squared_error']\r\n",
        "    },\r\n",
        "    'initialization':{\r\n",
        "        'values':['xavier','random']\r\n",
        "    },\r\n",
        "    'activation':{\r\n",
        "        'values':['relu','sigmoid','tanh']\r\n",
        "    },\r\n",
        "    'n_epoch':{\r\n",
        "        'values':[5]\r\n",
        "    }\r\n",
        "}\r\n",
        "\r\n",
        "sweep_config['parameters']=parameters_dict"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhzhNivBBleu"
      },
      "source": [
        "sweep_id=wandb.sweep(sweep_config,project=proj_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YKEltHeCKxR"
      },
      "source": [
        "wandb.agent(sweep_id,train,project=proj_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710,
          "referenced_widgets": [
            "7cff98faec14450dabd714823d4aeeca",
            "3f33a1091a0d4a29927274011789b4bd",
            "2597ae7ff0df4021a7b84ee4fd7545a4",
            "9fb4b710580040449f5de8b8a43ecf2b",
            "ace4e1fe861048f597908cf9a667ec7d",
            "f84a0f491480419c98242b6ba1afde38",
            "8a5e44906a504e1eb4fbccb5dae2f6ba",
            "948da8ab43ec4c05a7f7d92e80c61c72"
          ]
        },
        "id": "jPLrSpBhcwwx",
        "outputId": "ee178c59-18c4-40e0-d0f9-a66ba378947d"
      },
      "source": [
        "#confusion matrix and accuracy for test values\r\n",
        "hyperparameter_final=dict(\r\n",
        "    input_size = 784,                                       \r\n",
        "    output_size = 10,                                                    \r\n",
        "    n_epoch = 5,                                            \r\n",
        "    n_hiddenlayer = 3,                               \r\n",
        "    hl= [128,128,128],\r\n",
        "    reg = 0,      \r\n",
        "    lr = 1e-5,                                              \r\n",
        "    optimizer = \"nadam\",                      \r\n",
        "    batch_size = 128,       \r\n",
        "    initialization = \"xavier\",      \r\n",
        "    loss_type = \"cross_entropy\",\r\n",
        "    activation=\"relu\" \r\n",
        "    \r\n",
        ")\r\n",
        "\r\n",
        "wandb.init(config=hyperparameter_final,project=proj_name)\r\n",
        "config=wandb.config\r\n",
        "parameters_test=do_GD(X_train,Y_train,config.optimizer,config.activation,config.hl,config.input_size,config.output_size,config.n_epoch,config.lr,config.reg,config.initialization,config.loss_type,config.batch_size,logging=False)\r\n",
        "sizes = config.hl.copy() \r\n",
        "sizes.insert(0,config.input_size)\r\n",
        "sizes.append(config.output_size)\r\n",
        "\r\n",
        "Y_prob=np.empty(np.shape(Y_test))\r\n",
        "#finding y predicted\r\n",
        "for i,x in enumerate(X_test):\r\n",
        "  Y_prob[i]= feed_forward(x,parameters_test,sizes,config.activation)[0]\r\n",
        "\r\n",
        "accuracy=calcAccLoss(parameters,X_test,Y_test,sizes,None,config.activation)[0]\r\n",
        "\r\n",
        "wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=Y_prob, y_true=Y_test,class_names=class_list),\"Test Accuracy\": accuracy })"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:krrqx0cs) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 2662<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7cff98faec14450dabd714823d4aeeca",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210315_120634-krrqx0cs/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210315_120634-krrqx0cs/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">deep-frog-169</strong>: <a href=\"https://wandb.ai/rohithd/CS6910_ass1/runs/krrqx0cs\" target=\"_blank\">https://wandb.ai/rohithd/CS6910_ass1/runs/krrqx0cs</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "...Successfully finished last run (ID:krrqx0cs). Initializing new run:<br/><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.22<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">olive-sun-170</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/rohithd/CS6910_ass1\" target=\"_blank\">https://wandb.ai/rohithd/CS6910_ass1</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/rohithd/CS6910_ass1/runs/2aojfga6\" target=\"_blank\">https://wandb.ai/rohithd/CS6910_ass1/runs/2aojfga6</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210315_120729-2aojfga6</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-fe2e0bdc67ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyperparameter_final\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproj_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mparameters_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_GD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0msizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-585085f3e75c>\u001b[0m in \u001b[0;36mdo_GD\u001b[0;34m(X_train, Y_train, optimizer, activation, hl_size, input_size, output_size, n_epoch, lr, reg, w_init, loss_type, minibatch_size, logging)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madam_GD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"nadam\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnadam_GD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-6833c05cbe35>\u001b[0m in \u001b[0;36mnadam_GD\u001b[0;34m(X_train, Y_train, activation, n_epoch, sizes, lr, reg, w_init, loss_type, minibatch_size, log)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mupdate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreciprocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_w_hat\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mm_w_hat\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dW\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0mupdate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreciprocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_b_hat\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mm_b_hat\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"db\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvgmV7uQxsjs"
      },
      "source": [
        "#mnist dataset\r\n",
        "\r\n",
        "(x_train_mn, y_train_mn), (x_test_mn, y_test_mn) = fashion_mnist.load_data()\r\n",
        "\r\n",
        "X_train_mn = np.array(x_train_mn.reshape(x_train_mn.shape[0], 784,1))         # reshape 2-D data to 1-D\r\n",
        "X_test_mn = np.array(x_test_mn.reshape(x_test_mn.shape[0], 784,1))            # reshape 2-D data to 1-D\r\n",
        "\r\n",
        "X_train_mn = normalize_data(X_train_mn)\r\n",
        "#X_val = X_train_mn[-6000:]                                             # validation set input\r\n",
        "#X_train_mn = X_train_mn[0:54000]                                          # train_mning set input\r\n",
        "X_test_mn = normalize_data(X_test_mn)                                     # test_mn set input\r\n",
        "\r\n",
        "\r\n",
        "Y_train_mn = np.zeros([len(y_train_mn),10,1])\r\n",
        "Y_test_mn = np.zeros([len(y_test_mn),10,1])\r\n",
        "\r\n",
        "for i in range(len(y_train_mn)):                                        # convert y from just a class number to an indicator vector (10x1)\r\n",
        "  y = np.zeros([10, 1])\r\n",
        "  y[y_train_mn[i]] = 1.0\r\n",
        "  Y_train_mn[i] = y\r\n",
        "\r\n",
        "#Y_val = Y_train_mn[-6000:]                                              # validation set output\r\n",
        "#Y_train_mn = Y_train_mn[0:54000]                                           # train_mning set output\r\n",
        "\r\n",
        "for i in range(len(y_test_mn)):                                         # convert y from just a class number to an indicator vector (10x1)\r\n",
        "  y = np.zeros([10, 1])\r\n",
        "  y[y_test_mn[i]] = 1.0\r\n",
        "  Y_test_mn[i] = y          \r\n",
        "\r\n",
        "hyperparameter_final=dict(\r\n",
        "    input_size = 784,                                       \r\n",
        "    output_size = 10,                                                    \r\n",
        "    n_epoch = 5,                                            \r\n",
        "    n_hiddenlayer = 3,                               \r\n",
        "    hl= [64,64,64],\r\n",
        "    reg = 0.0005,      \r\n",
        "    lr = 1e-3,                                              \r\n",
        "    optimizer = \"nadam\",                      \r\n",
        "    batch_size = 64,       \r\n",
        "    initialization = \"xavier\",      \r\n",
        "    loss_type = \"cross_entropy\" \r\n",
        "    \r\n",
        ")\r\n",
        "\r\n",
        "wandb.init(config=hyperparameter_final,project=proj_name)\r\n",
        "sizes = config.hl.copy() \r\n",
        "sizes.insert(0,config.input_size)\r\n",
        "sizes.append(config.output_size)\r\n",
        "\r\n",
        "parameters_test=do_GD(X_train_mn,Y_train_mn,config.optimizer,config.activation,config.hl,config.input_size,config.output_size,config.n_epoch,config.lr,config.reg,config.initialization,config.loss_type,config.batch_size,logging==False)\r\n",
        "accuracy=calcAccLoss(parameters,X_test_mn,Y_test_mn,sizes,None,config.activation)[0]\r\n",
        "\r\n",
        "wandb.log({\"Test Accuracy\": accuracy })"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}