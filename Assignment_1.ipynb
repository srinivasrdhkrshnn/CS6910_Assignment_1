{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srinivasrdhkrshnn/CS6910_Assignment_1/blob/main/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRXXPzPKDf4V"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KptVZx1fm3E1"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from keras.datasets import fashion_mnist\r\n",
        "from keras.datasets import mnist\r\n",
        "import wandb\r\n",
        "\r\n",
        "# load dataset\r\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\r\n",
        "class_type = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'] \r\n",
        "\r\n",
        "proj_name='CS6910_ass1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUPA7aDORAu_"
      },
      "source": [
        "\r\n",
        "print(\"Sample Images for each Class :\")\r\n",
        "class_list=list()\r\n",
        "wandb.init(project=proj_name)\r\n",
        "for i in range(10):\r\n",
        "  plt.subplot(2,5,i+1)\r\n",
        "  for j in range(len(y_train)):\r\n",
        "    if y_train[j] == i :\r\n",
        "        wandb.log({\"img\": [wandb.Image(x_train[j], caption=class_type[y_train[j]])]})\r\n",
        "        class_list.append(class_type[y_train[j]])\r\n",
        "        break      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZ0U8UVcl9ZW"
      },
      "source": [
        "def activate(x,activation):                                                           # Hidden layer activation function                                  \n",
        "\n",
        "  if activation == \"sigmoid\":\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "  elif activation == \"tanh\": \n",
        "    return np.tanh(x) \n",
        "\n",
        "  elif activation == \"relu\": \n",
        "    return x * (x > 0) \n",
        "\n",
        "def softmax(x):                                                                       # Output activation function\n",
        "    return np.exp(x) / np.sum(np.exp(x)) \n",
        "\n",
        "def feed_forward(x,parameters,sizes,activation):                                # feed-forward data through the network to estimate output\n",
        "  \n",
        "  H = {}\n",
        "  A={}\n",
        "  H[0] = x\n",
        " \n",
        "  for i in range(1,len(sizes)-1):\n",
        "    W = parameters[\"W\"+str(i)]\n",
        "    b = parameters[\"b\"+str(i)]\n",
        "    A[i] = np.dot(W,H[i-1])+b\n",
        "    H[i] = activate(A[i],activation)\n",
        "    \n",
        "  W = parameters[\"W\"+str(len(sizes)-1)]\n",
        "  b = parameters[\"b\"+str(len(sizes)-1)]\n",
        "  A[len(sizes)-1] = np.dot(W,H[len(sizes)-2])+b\n",
        "  #print(\"A:\",A[2],\"H:\",H[3],len(sizes))\n",
        "\n",
        "  y_hat = softmax(A[len(sizes)-1])\n",
        "  #print(y_hat)\n",
        "  \n",
        "  return y_hat,A,H\n",
        "\n",
        "def loss_compute(y,y_hat,parameters,loss_type,reg,sizes):                                               # function to compute the loss/error (both squared error and cross entropy)\n",
        "\n",
        "  if (loss_type == \"squared_error\"):\n",
        "    error = np.sum((y-y_hat)**2)/(2*len(y))\n",
        "  elif (loss_type == \"cross_entropy\") :\n",
        "    error = -1*np.sum(np.multiply(y,np.log(y_hat)))/len(y)\n",
        "\t\t\n",
        "  reg_error = 0.0                                                                        # account for regularization to avoid overfit of data - L2 norm regularization\n",
        "  for i in range(1,len(sizes)) :\n",
        "    reg_error = reg_error + (reg/2)*(np.sum(np.square(parameters[\"W\"+str(i)]))) \n",
        "  error = error + reg_error\n",
        "\n",
        "  return error\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe9Q5RfeoV34"
      },
      "source": [
        "def derivative(x,activation):                                                            # function to compute the derivative of hidden layer activation fn\n",
        "\n",
        "  if activation == \"sigmoid\":\n",
        "    return activate(x,\"sigmoid\")*(1-activate(x,\"sigmoid\"))\n",
        "  elif activation == \"tanh\": \n",
        "    return 1. - x * x \n",
        "  elif activation == \"relu\": \n",
        "    return 1. * (x > 0)\n",
        "\n",
        "def grad_init(sizes):\n",
        "  \n",
        "  grads={}\n",
        "  layers=len(sizes)\n",
        "  for i in range(1,layers):\n",
        "    grads[\"dW\" + str(i)] = np.zeros((sizes[i], sizes[i-1]))\n",
        "    grads[\"db\" + str(i)] = np.zeros((sizes[i],1))\n",
        "\n",
        "  return grads  \n",
        "\n",
        "\n",
        "def back_prop(X,Y,Y_hat,prev_grad,A,H,parameters,sizes,loss_type,activation,reg) :               # back-propogation rule to compute the gradients of activation, pre-activation and parameters\n",
        "  \n",
        "  new_grad = {}\n",
        "  grads = {}\n",
        "  # grads = {\"dH0\":np.zeros((input_size,1)),\"dA0\":np.zeros((input_size,1))}\n",
        "  for i in range(1,len(sizes)):\n",
        "    grads[\"dW\" + str(i)] = np.zeros((sizes[i], sizes[i-1]))\n",
        "    grads[\"db\" + str(i)] = np.zeros((sizes[i],1))\n",
        "    grads[\"dA\" + str(i)] = np.zeros((sizes[i],1))\n",
        "    grads[\"dH\" + str(i)] = np.zeros((sizes[i],1))\n",
        "\n",
        "  if loss_type == \"squared_error\":\n",
        "    grads[\"dH\"+str(len(sizes)-1)] = (Y_hat-Y) \n",
        "    grads[\"dA\"+str(len(sizes)-1)] = (Y_hat - Y)*Y_hat - Y_hat*(np.dot(np.transpose((Y_hat-Y)), Y_hat))\n",
        "\n",
        "  elif loss_type==\"cross_entropy\" :\n",
        "    grads[\"dH\"+str(len(sizes)-1)] = -(Y/Y_hat) \n",
        "    grads[\"dA\"+str(len(sizes)-1)] = -(Y-Y_hat)\n",
        "\n",
        "  for i in range(len(sizes)-1, 0, -1):\n",
        "    grads[\"dW\" + str(i)] = np.dot(grads[\"dA\" + str(i)], np.transpose(H[i-1]))\n",
        "    grads[\"db\" + str(i)] = grads[\"dA\" + str(i)] \n",
        "    if i>1 :\n",
        "      grads[\"dH\" + str(i-1)] = np.dot(np.transpose(parameters[\"W\" + str(i)]), grads[\"dA\" + str(i)])\n",
        "      grads[\"dA\" + str(i-1)] = np.multiply((grads[\"dH\" + str(i-1)]),derivative(A[i-1],activation))\n",
        "    \n",
        "  for i in range(1,len(sizes)):\n",
        "    new_grad[\"dW\" + str(i)] = grads[\"dW\" + str(i)] + prev_grad[\"dW\" + str(i)]\n",
        "    new_grad[\"db\" + str(i)] = grads[\"db\" + str(i)] + prev_grad[\"db\" + str(i)]\n",
        "    \n",
        "  return new_grad  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OplHJZ91N5C0"
      },
      "source": [
        "# Initializations\r\n",
        "\r\n",
        "# Data\r\n",
        "X_train = np.array(x_train.reshape(x_train.shape[0], 784,1))         # reshape 2-D data to 1-D\r\n",
        "X_test = np.array(x_test.reshape(x_test.shape[0], 784,1))            # reshape 2-D data to 1-D\r\n",
        "\r\n",
        "def normalize_data(x):                                               # normalize input data\r\n",
        "  x_norm = x.astype('float32')\r\n",
        "  x_norm = x_norm / 255.0  \r\n",
        "  return x_norm \r\n",
        "\r\n",
        "X_train = normalize_data(X_train)\r\n",
        "X_val = X_train[-6000:]                                             # validation set input\r\n",
        "X_train = X_train[0:54000]                                          # training set input\r\n",
        "X_test = normalize_data(X_test)                                     # test set input\r\n",
        "\r\n",
        "\r\n",
        "Y_train = np.zeros([len(y_train),10,1])\r\n",
        "Y_test = np.zeros([len(y_test),10,1])\r\n",
        "\r\n",
        "for i in range(len(y_train)):                                        # convert y from just a class number to an indicator vector (10x1)\r\n",
        "  y = np.zeros([10, 1])\r\n",
        "  y[y_train[i]] = 1.0\r\n",
        "  Y_train[i] = y\r\n",
        "\r\n",
        "Y_val = Y_train[-6000:]                                              # validation set output\r\n",
        "Y_train = Y_train[0:54000]                                           # training set output\r\n",
        "\r\n",
        "for i in range(len(y_test)):                                         # convert y from just a class number to an indicator vector (10x1)\r\n",
        "  y = np.zeros([10, 1])\r\n",
        "  y[y_test[i]] = 1.0\r\n",
        "  Y_test[i] = y                                                      # test set output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYuLb65aZAVr"
      },
      "source": [
        "def network_init(sizes,w_init):                            # function to initialize weights and biases\r\n",
        "  parameters = {}\r\n",
        "  for i in range(1, len(sizes)):\r\n",
        "    if w_init == \"xavier\" :\r\n",
        "      parameters[\"W\" + str(i)] = np.random.randn(sizes[i], sizes[i-1])*np.sqrt(2./(sizes[i] + sizes[i-1]))\r\n",
        "      parameters[\"b\" + str(i)] = np.zeros((sizes[i],1))\r\n",
        "    elif w_init == \"random\" :\r\n",
        "      parameters[\"W\" + str(i)] = 0.01*np.random.randn(sizes[i], sizes[i-1])\r\n",
        "      parameters[\"b\" + str(i)] = 0.01*np.random.randn(sizes[i],1)\r\n",
        "\r\n",
        "  return parameters  \r\n",
        "\r\n",
        "def update_init(sizes) :                                  # function to initialize update dictionary that changes the weights and biases\r\n",
        "  update = {}\r\n",
        "  for i in range(1,len(sizes)):\r\n",
        "   update[\"W\"+str(i)] = np.zeros((sizes[i],sizes[i-1]))\r\n",
        "   update[\"b\"+str(i)] = np.zeros((sizes[i],1))\r\n",
        "\r\n",
        "  return update"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJTGRW9sb2C7"
      },
      "source": [
        "def val_loss(y,y_hat,loss_type):                                # function to compute the loss/error (both squared error and cross entropy)\r\n",
        "  l = 0\r\n",
        "  if (loss_type == \"squared_error\"):\r\n",
        "    l = np.sum((y-y_hat)**2)/(2*len(y))\r\n",
        "  elif (loss_type == \"cross_entropy\") :\r\n",
        "    l = -1*np.sum(np.multiply(y,np.log(y_hat)))/len(y)\r\n",
        "  return l \r\n",
        "\r\n",
        "def calcAccLoss(parameters,xArr,yArr,sizes,loss_type,activation,type=\"val\",regu=None):          #function to calculate accuracy and total loss of a model\r\n",
        "  acc=0.0\r\n",
        "  lossVal=0.0\r\n",
        "  for x,y in zip(xArr,yArr):\r\n",
        "    y_hat= feed_forward(x,parameters,sizes,activation)[0]\r\n",
        "    if y_hat.argmax()==y.argmax():\r\n",
        "      acc+=1\r\n",
        "    if type==\"val\":\r\n",
        "      lossVal+=val_loss(y,y_hat,loss_type)\r\n",
        "    elif type==\"trng\":\r\n",
        "      lossVal+=loss_compute(y,y_hat,parameters,loss_type,regu,sizes)\r\n",
        "  acc=acc/len(xArr)\r\n",
        "  return (acc,lossVal)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "getumZEKyShg"
      },
      "source": [
        "#momentum Gradient descent\n",
        "def momentum_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=False) :\n",
        "\n",
        "   steps=0                                                                                      #used to count number of updates to parameters\n",
        "   parameters = network_init(sizes,w_init)\n",
        "   update = update_init(sizes)\n",
        "   gamma = 0.9\n",
        "   for n in range(n_epoch):\n",
        "\n",
        "     for j in range(0, X_train.shape[0], minibatch_size):                                       #minibatch division \n",
        "       X_mini = X_train[j:j + minibatch_size]\n",
        "       Y_mini = Y_train[j:j + minibatch_size]\n",
        "       grads = grad_init(sizes)\n",
        "\n",
        "       for x,y in zip(X_mini,Y_mini):\n",
        "         y_hat,A,H = feed_forward(x,parameters,sizes,activation)                                 \n",
        "         grads = back_prop(x,y,y_hat,grads,A,H,parameters,sizes,loss_type,activation,reg)\n",
        "\n",
        "       for i in range(1,len(sizes)) :                                                           #updating the parameters\n",
        "         update[\"W\"+str(i)] = gamma*update[\"W\"+str(i)] + lr*grads[\"dW\"+str(i)]                  \n",
        "         update[\"b\"+str(i)] = gamma*update[\"b\"+str(i)] + lr*grads[\"db\"+str(i)]\n",
        "         parameters[\"W\"+str(i)] = (1-lr*reg)*parameters[\"W\"+str(i)] - update[\"W\"+str(i)]\n",
        "         parameters[\"b\"+str(i)] = (1-lr*reg)*parameters[\"b\"+str(i)] - update[\"b\"+str(i)]\n",
        "       \n",
        "       steps=steps+1\n",
        "       if steps==10000:                                                                         #log every 10000 updates\n",
        "        acc,lossTot=calcAccLoss(parameters,X_train,Y_train,sizes,loss_type,activation,type=\"trng\",regu=reg)          #calculating accuracy and loss\n",
        "        accVal,lossTotVal=calcAccLoss(parameters,X_val,Y_val,sizes,loss_type,activation)\n",
        "\n",
        "        wandb.log({\"Accuracy\":acc,\"Loss\":lossTot,\"Accuracy_val\":accVal,\"Loss_val\":lossTotVal,\"Epoch\":n,\"n_datatrain\":j + minibatch_size+n*54000}) #logging\n",
        "        steps=0\n",
        "  \n",
        "   return parameters        \n",
        "\n",
        "#Nesterov acceleracted GD\n",
        "def nesterov_accelerated_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=False) :\n",
        "  steps=0\n",
        "  parameters = network_init(sizes,w_init)\n",
        "  update = update_init(sizes)\n",
        "  gamma = 0.9\n",
        "  for n in range(n_epoch):\n",
        "\n",
        "    for j in range(0, X_train.shape[0], minibatch_size):\n",
        "      X_mini = X_train[j:j + minibatch_size]\n",
        "      Y_mini = Y_train[j:j + minibatch_size]\n",
        "      \n",
        "\n",
        "      grads = grad_init(sizes)\n",
        "      for i in range(1,len(sizes)):                                                       #perform update before back propogation\n",
        "        update[\"W\"+str(i)] = gamma*update[\"W\"+str(i)]\n",
        "        update[\"b\"+str(i)] = gamma*update[\"b\"+str(i)]\n",
        "        parameters[\"W\"+str(i)] = (1-lr*reg)*parameters[\"W\"+str(i)] - update[\"W\"+str(i)]\n",
        "        parameters[\"b\"+str(i)] = (1-lr*reg)*parameters[\"b\"+str(i)] - update[\"b\"+str(i)]        \n",
        "\n",
        "      for x,y in zip(X_mini,Y_mini):\n",
        "        y_hat,A,H = feed_forward(x,parameters,sizes,activation)\n",
        "        grads = back_prop(x,y,y_hat,grads,A,H,parameters,sizes,loss_type,activation,reg)\n",
        "        \n",
        "      for k in range(1,len(sizes)) :\n",
        "        update[\"W\"+str(k)] = gamma*update[\"W\"+str(k)] + lr*grads[\"dW\"+str(k)]\n",
        "        update[\"b\"+str(k)] = gamma*update[\"b\"+str(k)] + lr*grads[\"db\"+str(k)]\n",
        "        parameters[\"W\"+str(k)] = (1-lr*reg)*parameters[\"W\"+str(k)] - update[\"W\"+str(k)]\n",
        "        parameters[\"b\"+str(k)] = (1-lr*reg)*parameters[\"b\"+str(k)] - update[\"b\"+str(k)]\n",
        "      steps=steps+1\n",
        "      if steps==10000:\n",
        "        if log:\n",
        "          acc,lossTot=calcAccLoss(parameters,X_train,Y_train,sizes,loss_type,activation,type=\"trng\",regu=reg)     \n",
        "          accVal,lossTotVal=calcAccLoss(parameters,X_val,Y_val,sizes,loss_type,activation)\n",
        "\n",
        "          wandb.log({\"Accuracy\":acc,\"Loss\":lossTot,\"Accuracy_val\":accVal,\"Loss_val\":lossTotVal,\"Epoch\":n,\"n_datatrain\":j + minibatch_size+n*54000})\n",
        "        steps=0   \n",
        "    \n",
        "  return parameters\n",
        "\n",
        "#stochastic GD\n",
        "def stochastic_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=False) :\n",
        "\n",
        "  steps=0\n",
        "  parameters = network_init(sizes,w_init)\n",
        "  update = update_init(sizes)\n",
        "  gamma = 0.9\n",
        "  for n in range(n_epoch):\n",
        "\n",
        "    for j in range(0, X_train.shape[0], minibatch_size):                                        #minibatch division\n",
        "      X_mini = X_train[j:j + minibatch_size]\n",
        "      Y_mini = Y_train[j:j + minibatch_size]\n",
        "      \n",
        "      grads = grad_init(sizes)\n",
        "      for x,y in zip(X_mini,Y_mini):\n",
        "        y_hat,A,H = feed_forward(x,parameters,sizes,activation)\n",
        "        grads = back_prop(x,y,y_hat,grads,A,H,parameters,sizes,loss_type,activation,reg)\n",
        "        \n",
        "      for i in range(1,len(sizes)-1) :                                                          #updating the parameters\n",
        "        parameters[\"W\"+str(i)] = (1-lr*reg)*parameters[\"W\"+str(i)] - lr*grads[\"dW\"+str(i)]\n",
        "        parameters[\"b\"+str(i)] = (1-lr*reg)*parameters[\"b\"+str(i)] - lr*grads[\"db\"+str(i)]\n",
        "      steps=steps+1\n",
        "      if steps==10000:\n",
        "        if log:\n",
        "          acc,lossTot=calcAccLoss(parameters,X_train,Y_train,sizes,loss_type,activation,type=\"trng\",regu=reg)     \n",
        "          accVal,lossTotVal=calcAccLoss(parameters,X_val,Y_val,sizes,loss_type,activation)\n",
        "\n",
        "          wandb.log({\"Accuracy\":acc,\"Loss\":lossTot,\"Accuracy_val\":accVal,\"Loss_val\":lossTotVal,\"Epoch\":n,\"n_datatrain\":j + minibatch_size+n*54000})\n",
        "        steps=0   \n",
        "\n",
        "  return parameters        \n",
        "\n",
        "#rmsprop GD\n",
        "def rmsprop_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=False) :\n",
        "  \n",
        "  steps=0\n",
        "  parameters = network_init(sizes,w_init)\n",
        "  update = update_init(sizes)\n",
        "  v = update_init(sizes)\n",
        "\n",
        "  betal = 0.99 #check this\n",
        "  eps = 1e-8\n",
        "\n",
        "  for n in range(n_epoch):\n",
        "\n",
        "    for j in range(0, X_train.shape[0], minibatch_size):\n",
        "      X_mini = X_train[j:j + minibatch_size]\n",
        "      Y_mini = Y_train[j:j + minibatch_size]\n",
        "      \n",
        "      grads = grad_init(sizes)\n",
        "      for x,y in zip(X_mini,Y_mini):\n",
        "        y_hat,A,H = feed_forward(x,parameters,sizes,activation)\n",
        "        grads = back_prop(x,y,y_hat,grads,A,H,parameters,sizes,loss_type,activation,reg)\n",
        "\n",
        "      for i in range(1,len(sizes)-1) :                                                                   #updating the parameters\n",
        "        v[\"W\"+str(i)] = betal*v[\"W\"+str(i)] + (1-betal)*grads[\"dW\"+str(i)]**2                            #v_w update  \n",
        "        v[\"b\"+str(i)] = betal*v[\"b\"+str(i)] + (1-betal)*grads[\"db\"+str(i)]**2                            #v_b update\n",
        "\n",
        "        update[\"W\"+str(i)]=lr*np.multiply(np.reciprocal(np.sqrt(v[\"W\"+str(i)]+eps)),grads[\"dW\"+str(i)])                 \n",
        "        update[\"b\"+str(i)]=lr*np.multiply(np.reciprocal(np.sqrt(v[\"b\"+str(i)]+eps)),grads[\"db\"+str(i)])\n",
        "        \n",
        "        parameters[\"W\"+str(i)] = (1-lr*reg)*parameters[\"W\"+str(i)] - update[\"W\"+str(i)]\n",
        "        parameters[\"b\"+str(i)] = (1-lr*reg)*parameters[\"b\"+str(i)] - update[\"b\"+str(i)]\n",
        "      steps=steps+1\n",
        "      if steps==10000:\n",
        "        if log:\n",
        "          acc,lossTot=calcAccLoss(parameters,X_train,Y_train,sizes,loss_type,activation,type=\"trng\",regu=reg)     \n",
        "          accVal,lossTotVal=calcAccLoss(parameters,X_val,Y_val,sizes,loss_type,activation)\n",
        "\n",
        "          wandb.log({\"Accuracy\":acc,\"Loss\":lossTot,\"Accuracy_val\":accVal,\"Loss_val\":lossTotVal,\"Epoch\":n,\"n_datatrain\":j + minibatch_size+n*54000})\n",
        "        steps=0   \n",
        "\n",
        "  return parameters          \n",
        "\n",
        "#Adam GD\n",
        "def adam_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=False) :\n",
        "  steps=0\n",
        "  parameters = network_init(sizes,w_init)\n",
        "  update = update_init(sizes)\n",
        "  m = update_init(sizes)\n",
        "  v = update_init(sizes)\n",
        "\n",
        "  beta1 = 0.9\n",
        "  beta2 = 0.999\n",
        "  eps = 1e-8\n",
        "  for n in range(n_epoch):\n",
        "\n",
        "    for j in range(0, X_train.shape[0], minibatch_size):\n",
        "      X_mini = X_train[j:j + minibatch_size]\n",
        "      Y_mini = Y_train[j:j + minibatch_size]\n",
        "      \n",
        "      grads = grad_init(sizes)                                          \n",
        "      for x,y in zip(X_mini,Y_mini):\n",
        "        y_hat,A,H = feed_forward(x,parameters,sizes,activation)\n",
        "        grads = back_prop(x,y,y_hat,grads,A,H,parameters,sizes,loss_type,activation,reg)\n",
        "        \n",
        "      for i in range(1,len(sizes)-1) :                                                    #updating the parameters\n",
        "        m[\"W\"+str(i)] = beta1*m[\"W\"+str(i)] + (1-beta1)*grads[\"dW\"+str(i)]                #m_w update\n",
        "        m[\"b\"+str(i)] = beta1*m[\"b\"+str(i)] + (1-beta1)*grads[\"db\"+str(i)]                #m_b update\n",
        "\n",
        "        v[\"W\"+str(i)] = beta2*v[\"W\"+str(i)] + (1-beta2)*grads[\"dW\"+str(i)]**2             #v_w update    \n",
        "        v[\"b\"+str(i)] = beta2*v[\"b\"+str(i)] + (1-beta2)*grads[\"db\"+str(i)]**2             #v_b update\n",
        "\n",
        "        #cumulative average\n",
        "        m_w_hat = m[\"W\"+str(i)]/(1-np.power(beta1,n+1))                                   \n",
        "        m_b_hat = m[\"b\"+str(i)]/(1-np.power(beta1,n+1))\n",
        "        v_w_hat = v[\"W\"+str(i)]/(1-np.power(beta2,n+1))\n",
        "        v_b_hat = v[\"b\"+str(i)]/(1-np.power(beta2,n+1))\n",
        "\n",
        "\n",
        "        update[\"W\"+str(i)]=lr*np.multiply(np.reciprocal(np.sqrt(v_w_hat+eps)),m_w_hat)\n",
        "        update[\"b\"+str(i)]=lr*np.multiply(np.reciprocal(np.sqrt(v_b_hat+eps)),m_b_hat)\n",
        "\n",
        "        parameters[\"W\"+str(i)] = (1-lr*reg)*parameters[\"W\"+str(i)] - update[\"W\"+str(i)]\n",
        "        parameters[\"b\"+str(i)] = (1-lr*reg)*parameters[\"b\"+str(i)] - update[\"b\"+str(i)]\n",
        "      \n",
        "    \n",
        "      steps=steps+1\n",
        "      if steps==10000:\n",
        "        if log:\n",
        "          acc,lossTot=calcAccLoss(parameters,X_train,Y_train,sizes,loss_type,activation,type=\"trng\",regu=reg)     \n",
        "          accVal,lossTotVal=calcAccLoss(parameters,X_val,Y_val,sizes,loss_type,activation)\n",
        "\n",
        "          wandb.log({\"Accuracy\":acc,\"Loss\":lossTot,\"Accuracy_val\":accVal,\"Loss_val\":lossTotVal,\"Epoch\":n,\"n_datatrain\":j + minibatch_size+n*54000})\n",
        "        steps=0     \n",
        "\n",
        "  return parameters   \n",
        "\n",
        "\n",
        "#Nadam GD\n",
        "def nadam_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=False) :\n",
        "  steps=0\n",
        "  parameters = network_init(sizes,w_init)\n",
        "  update = update_init(sizes)\n",
        "  m = update_init(sizes)\n",
        "  v = update_init(sizes)\n",
        "\n",
        "  beta1 = 0.9\n",
        "  beta2 = 0.999\n",
        "  eps = 1e-8\n",
        "\n",
        "  for n in range(n_epoch):\n",
        "\n",
        "    for j in range(0, X_train.shape[0], minibatch_size):\n",
        "      X_mini = X_train[j:j + minibatch_size]\n",
        "      Y_mini = Y_train[j:j + minibatch_size]\n",
        "      grads = grad_init(sizes)\n",
        "\n",
        "      for x,y in zip(X_mini,Y_mini):\n",
        "        y_hat,A,H = feed_forward(x,parameters,sizes,activation)\n",
        "        grads = back_prop(x,y,y_hat,grads,A,H,parameters,sizes,loss_type,activation,reg)\n",
        "\n",
        "      for i in range(1,len(sizes)-1) :                                                    #updating the parameters\n",
        "        m[\"W\"+str(i)] = beta1*m[\"W\"+str(i)] + (1-beta1)*grads[\"dW\"+str(i)]                #m_w update\n",
        "        m[\"b\"+str(i)] = beta1*m[\"b\"+str(i)] + (1-beta1)*grads[\"db\"+str(i)]                #m_b update\n",
        "\n",
        "        v[\"W\"+str(i)] = beta2*v[\"W\"+str(i)] + (1-beta2)*grads[\"dW\"+str(i)]**2             #v_w update    \n",
        "        v[\"b\"+str(i)] = beta2*v[\"b\"+str(i)] + (1-beta2)*grads[\"db\"+str(i)]**2             #v_b update\n",
        "\n",
        "        #cumulative average\n",
        "        m_w_hat = m[\"W\"+str(i)]/(1-np.power(beta1,n+1))                                   \n",
        "        m_b_hat = m[\"b\"+str(i)]/(1-np.power(beta1,n+1))\n",
        "        v_w_hat = v[\"W\"+str(i)]/(1-np.power(beta2,n+1))\n",
        "        v_b_hat = v[\"b\"+str(i)]/(1-np.power(beta2,n+1))\n",
        "\n",
        "\n",
        "        update[\"W\"+str(i)]=lr*np.multiply(np.reciprocal(np.sqrt(v_w_hat+eps)),(beta1*m_w_hat+(1-beta1)*grads[\"dW\"+str(i)]))*(1/(1-np.power(beta1,n+1)))\n",
        "        update[\"b\"+str(i)]=lr*np.multiply(np.reciprocal(np.sqrt(v_b_hat+eps)),(beta1*m_b_hat+(1-beta1)*grads[\"db\"+str(i)]))*(1/(1-np.power(beta1,n+1)))\n",
        "\n",
        "        parameters[\"W\"+str(i)] = (1-lr*reg)*parameters[\"W\"+str(i)] - update[\"W\"+str(i)]\n",
        "        parameters[\"b\"+str(i)] = (1-lr*reg)*parameters[\"b\"+str(i)] - update[\"b\"+str(i)]\n",
        "      steps=steps+1\n",
        "      if steps==10000:\n",
        "        if log:\n",
        "          acc,lossTot=calcAccLoss(parameters,X_train,Y_train,sizes,loss_type,activation,type=\"trng\",regu=reg)     \n",
        "          accVal,lossTotVal=calcAccLoss(parameters,X_val,Y_val,sizes,loss_type,activation)\n",
        "\n",
        "          wandb.log({\"Accuracy\":acc,\"Loss\":lossTot,\"Accuracy_val\":accVal,\"Loss_val\":lossTotVal,\"Epoch\":n,\"n_datatrain\":j + minibatch_size+n*54000})\n",
        "        steps=0   \n",
        "\n",
        "  return parameters   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SvODi-MsNvw"
      },
      "source": [
        "#function to select optimizer\r\n",
        "def do_GD(X_train,Y_train,optimizer,activation,hl_size,input_size,output_size,n_epoch,lr,reg,w_init,loss_type,minibatch_size=1,logging=False):\r\n",
        "  sizes = hl_size.copy() \r\n",
        "  sizes.insert(0,input_size)\r\n",
        "  sizes.append(output_size)\r\n",
        "\r\n",
        "  if optimizer==\"sgd\":\r\n",
        "    return(stochastic_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=logging))\r\n",
        "  elif optimizer==\"momentum\":\r\n",
        "    return(momentum_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=logging))\r\n",
        "  elif optimizer==\"nesterov\":\r\n",
        "    return(nesterov_accelerated_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=logging))\r\n",
        "  elif optimizer==\"rmsprop\":\r\n",
        "    return(rmsprop_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=logging))\r\n",
        "  elif optimizer==\"adam\":\r\n",
        "    return(adam_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=logging))\r\n",
        "  elif optimizer==\"nadam\":\r\n",
        "    return(nadam_GD(X_train,Y_train,activation,n_epoch,sizes,lr,reg,w_init,loss_type,minibatch_size=1,log=logging))\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOn63t--FAVJ"
      },
      "source": [
        "#training function to sweep with wandb\r\n",
        "def train():\r\n",
        "\r\n",
        "  hyperparameter_defaults=dict(\r\n",
        "      input_size = 784,                                       \r\n",
        "      output_size = 10,                                                    \r\n",
        "      n_epoch = 5,                                            \r\n",
        "      n_hiddenlayer = 3,                               \r\n",
        "      hl= [64,64,64],\r\n",
        "      reg = 0.0005,      \r\n",
        "      lr = 1e-3,                                              \r\n",
        "      optimizer = \"momentum\",                      \r\n",
        "      batch_size = 64,       \r\n",
        "      initialization = \"xavier\",      \r\n",
        "      loss_type = \"cross_entropy\" \r\n",
        "      \r\n",
        "  )\r\n",
        "\r\n",
        "  wandb.init(config=hyperparameter_defaults)\r\n",
        "\r\n",
        "  config=wandb.config\r\n",
        "  output_size=10\r\n",
        "  input_size = 784                                      \r\n",
        "  config.hl=[config.hl_size for i in range(config.n_hiddenlayer)]   #hidden layer sizes array creation\r\n",
        "  parameters=do_GD(X_train,Y_train,config.optimizer,config.activation,config.hl,config.input_size,config.output_size,config.n_epoch,config.lr,config.reg,config.initialization,config.loss_type,config.batch_size,logging=True)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jwiqh--NmGYS"
      },
      "source": [
        "def sweeper(sweep_config,proj_name):\r\n",
        "  sweep_id=wandb.sweep(sweep_config,project=proj_name)\r\n",
        "  wandb.agent(sweep_id,train,project=proj_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88hU77dlo4-O"
      },
      "source": [
        "#sweep dictionary\r\n",
        "sweep_config={\r\n",
        "    'method':'bayes',\r\n",
        "    'metric':{\r\n",
        "        'name':'accuracy',\r\n",
        "        'goal':'maximize'},\r\n",
        "\r\n",
        "}\r\n",
        "\r\n",
        "parameters_dict={\r\n",
        "    'optimizer':{\r\n",
        "        'values':['nadam','sgd', 'momentum', 'nesterov', 'rmsprop', 'adam']\r\n",
        "    },\r\n",
        "    'lr':{\r\n",
        "        'values':[1e-3,1e-5]\r\n",
        "    },\r\n",
        "    'reg':{\r\n",
        "        'values':[5e-4,0,5e-1]\r\n",
        "    },\r\n",
        "    'n_hiddenlayer':{\r\n",
        "        'values':[3,4,5]\r\n",
        "    },\r\n",
        "    'hl_size':{\r\n",
        "      'values':[128,32,64]  \r\n",
        "    },\r\n",
        "    'batch_size':{\r\n",
        "        'values':[64,32,128]\r\n",
        "    },\r\n",
        "    'loss_type':{\r\n",
        "        'values':['cross_entropy','squared_error']\r\n",
        "    },\r\n",
        "    'initialization':{\r\n",
        "        'values':['xavier','random']\r\n",
        "    },\r\n",
        "    'activation':{\r\n",
        "        'values':['relu','sigmoid','tanh']\r\n",
        "    },\r\n",
        "    'n_epoch':{\r\n",
        "        'values':[5]\r\n",
        "    }\r\n",
        "}\r\n",
        "\r\n",
        "sweep_config['parameters']=parameters_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhzhNivBBleu"
      },
      "source": [
        "sweep_id=wandb.sweep(sweep_config,project=proj_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo9jwT_kkZdL"
      },
      "source": [
        "sweeper(sweep_config,proj_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HfJ1njnLnM4"
      },
      "source": [
        "#confusion matrix and accuracy for test values\n",
        "hyperparameter_final=dict(\n",
        "    input_size = 784,                                       \n",
        "    output_size = 10,                                                    \n",
        "    n_epoch = 5,                                            \n",
        "    n_hiddenlayer = 3,                               \n",
        "    hl= [128,128,128],\n",
        "    reg = 0,      \n",
        "    lr = 1e-5,                                              \n",
        "    optimizer = \"nadam\",                      \n",
        "    batch_size = 128,       \n",
        "    initialization = \"xavier\",      \n",
        "    loss_type = \"cross_entropy\",\n",
        "    activation=\"relu\" \n",
        "    \n",
        ")\n",
        "\n",
        "wandb.init(config=hyperparameter_final,project=proj_name)\n",
        "config=wandb.config\n",
        "parameters_test=do_GD(X_train,Y_train,config.optimizer,config.activation,config.hl,config.input_size,config.output_size,config.n_epoch,config.lr,config.reg,config.initialization,config.loss_type,config.batch_size,logging=False)\n",
        "\n",
        "sizes = config.hl.copy() \n",
        "sizes.insert(0,config.input_size)\n",
        "sizes.append(config.output_size)\n",
        "\n",
        "Y_prob=np.empty(np.shape(y_test))\n",
        "#finding y predicted\n",
        "for i,x in enumerate(X_test):\n",
        "  Y_prob[i]= (feed_forward(x,parameters_test,sizes,config.activation)[0]).argmax()\n",
        "\n",
        "accuracy=calcAccLoss(parameters_test,X_test,Y_test,sizes,None,config.activation)[0]\n",
        "\n",
        "wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(preds=Y_prob, y_true=y_test,class_names=class_list),\"Test Accuracy\": accuracy })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvgmV7uQxsjs"
      },
      "source": [
        "#mnist dataset\r\n",
        "\r\n",
        "(x_train_mn, y_train_mn), (x_test_mn, y_test_mn) = mnist.load_data()\r\n",
        "\r\n",
        "X_train_mn = np.array(x_train_mn.reshape(x_train_mn.shape[0], 784,1))         # reshape 2-D data to 1-D\r\n",
        "X_test_mn = np.array(x_test_mn.reshape(x_test_mn.shape[0], 784,1))            # reshape 2-D data to 1-D\r\n",
        "\r\n",
        "X_train_mn = normalize_data(X_train_mn)\r\n",
        "#X_val = X_train_mn[-6000:]                                             # validation set input\r\n",
        "#X_train_mn = X_train_mn[0:54000]                                          # train_mning set input\r\n",
        "X_test_mn = normalize_data(X_test_mn)                                     # test_mn set input\r\n",
        "\r\n",
        "\r\n",
        "Y_train_mn = np.zeros([len(y_train_mn),10,1])\r\n",
        "Y_test_mn = np.zeros([len(y_test_mn),10,1])\r\n",
        "\r\n",
        "for i in range(len(y_train_mn)):                                        # convert y from just a class number to an indicator vector (10x1)\r\n",
        "  y = np.zeros([10, 1])\r\n",
        "  y[y_train_mn[i]] = 1.0\r\n",
        "  Y_train_mn[i] = y\r\n",
        "\r\n",
        "#Y_val = Y_train_mn[-6000:]                                              # validation set output\r\n",
        "#Y_train_mn = Y_train_mn[0:54000]                                           # train_mning set output\r\n",
        "\r\n",
        "for i in range(len(y_test_mn)):                                         # convert y from just a class number to an indicator vector (10x1)\r\n",
        "  y = np.zeros([10, 1])\r\n",
        "  y[y_test_mn[i]] = 1.0\r\n",
        "  Y_test_mn[i] = y          \r\n",
        "\r\n",
        "hyperparameter_final=dict(\r\n",
        "    input_size = 784,                                       \r\n",
        "    output_size = 10,                                                    \r\n",
        "    n_epoch = 5,                                            \r\n",
        "    n_hiddenlayer = 3,                               \r\n",
        "    hl= [128,128,128],\r\n",
        "    reg = 5e-4,      \r\n",
        "    lr = 1e-3,                                              \r\n",
        "    optimizer = \"adam\",                      \r\n",
        "    batch_size = 128,       \r\n",
        "    initialization = \"random\",      \r\n",
        "    loss_type = \"cross_entropy\",\r\n",
        "    activation=\"sigmoid\"  \r\n",
        "    \r\n",
        ")\r\n",
        "\r\n",
        "wandb.init(config=hyperparameter_final,project=proj_name)\r\n",
        "config=wandb.config\r\n",
        "sizes = config.hl.copy() \r\n",
        "sizes.insert(0,config.input_size)\r\n",
        "sizes.append(config.output_size)\r\n",
        "\r\n",
        "parameters_test=do_GD(X_train_mn,Y_train_mn,config.optimizer,config.activation,config.hl,config.input_size,config.output_size,config.n_epoch,config.lr,config.reg,config.initialization,config.loss_type,config.batch_size,logging=False)\r\n",
        "accuracy=calcAccLoss(parameters_test,X_test_mn,Y_test_mn,sizes,None,config.activation)[0]\r\n",
        "\r\n",
        "wandb.log({\"Test Accuracy\": accuracy })"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}